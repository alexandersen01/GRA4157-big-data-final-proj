\documentclass{article}
\usepackage{graphicx}
\usepackage{eurosym}
\usepackage[a4paper, total={6in, 8in}, margin=1in]{geometry}
\usepackage{listings}
\usepackage{times}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{float}
\usepackage{parskip}
\usepackage{minted}
\usepackage{fontawesome}
\usepackage{natbib}


\usemintedstyle{vs}
\newcommand{\ts}{\textsuperscript}

\title{Final assignment \\ GRA4157}

\author{
  \textbf{Jakob Sverre Alexandersen}
%   \thanks{This script is so incredibly baller omd} \\
%   FSN Labs \\
\\[1em]
  \href{mailto:ja.al@fsncapital.com}{\texttt{ja.al@fsncapital.com}} \\[1em]
}

\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}


% \vspace{2cm}
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.4\linewidth]{fsn_labs.png}
% \end{figure}

\newpage

\tableofcontents

\newpage

\section{Introduction}

Click me \href{https://github.com/alexandersen01/GRA4157-big-data-final-proj}{\faGithub} to view the full code on Github!

\noindent\rule{\textwidth}{0.4pt}

This final project builds (conceptually) on my second project. A short recap of that was to go in-depth on how CNNs (Convolutional Neural Networks) are actually built. To demonstrate this, I built a relatively simple classifier (found in the \verb|simple_bootleg_cnn_classifier.py| file). Furthermore, I ran it on the well-known MNIST dataset, which is a balanced dataset of $\sim 21,000$ images of the digits 0 to 9. It achieved both an $F1 $ and accuracy score of $99\% $, which is considered industry standard. Since the problem is considered solved, it inspires very little motivation to do further research on the subject. 

In my previous project, I very briefly mentioned Belkin et. al's theory of \textit{double descent} \cite{Belkin_2019}, which challenges the traditional bias-variance tradeoff. The theory posits that test error exhibits non-monotonic relationship with model complexity and training time: performance initially worsens after the interpolation threshold (where the model perfectly fits training data), but surprisingly improves again with further increases in complexity or training epochs. However, MNIST's simplicity, with near-perfect accuracy already achieved, provides limited opportunity to empirically observe this phenomenon. 

To meaningfully test double descent, I require a more challenging dataset where my model has substantial room for improvement. Therefore I will apply my CNN architecture to the CIFAR-10 dataset, which consists of $60,000 $ $32 \times 32 $ color images across 10 classes (airplane, car, bird, cat, deer, dog, frog, horse, ship and truck). CIFAR-10 presents significantly greater complexity due to color channels, natural image variations, and inter-class similarity, typically yielding baseline accuracies of $70 - 85\% $ without advanced techniques. 

The objectives of this project are therefore twofold: \textbf{(1)} to investigate whether double descent can be empirically observed by varying model complexity (number of layers, filters, and parameters), and \textbf{(2)} to examine epoch-wise double descent by training models well beyond typical convergence points. This exploration will provide practical insights into modern deep learning phenomena that remain theoretically incomplete. 

Remember to mention that I ran into an OOM issue, which is why I oved the validation through the network in batches to avoid the out-of-memory issue. the RTX is not that goated. heck ...

\newpage

\section{Methodology}

\subsection{Data}

Before we dive into the architectures of the different models and other techincals, it is important to achieve an understanding of the data itself. Therefore we start off by seeing how the distribution of the data looks like, to ensure that the data is indeed balanced.  

\begin{figure}[h!]
  \centering 
  \includegraphics[width=1\textwidth]{figures/image.png}
\end{figure}

The data is balanced, which means that we will not accidentally train a biased model that performs poorly on underrepresented classes. If that were the case, the model may have struggled to correctly identify the classes with fewer examples, potentially resulting in lower accuracy and recall for those classes. 

If we run a $t$SNE analysis on the test set, we get this plot: 

\begin{figure}[h!]
  \centering 
  \includegraphics[width=0.8\textwidth]{figures/image3.png}
\end{figure}

I can already tell from here that the model is not going spark impressive feedback, as the classes are very mixed into each other, and there is no clear separation between clusters. (TALK MORE ABOUT TSNE AND THIS IF ROOM ALSO ADD PICTURE OF IMAGES)

\newpage

\section{Results}

\newpage

\section{Summary}

\bibliographystyle{alpha}
\bibliography{sources}


\end{document}